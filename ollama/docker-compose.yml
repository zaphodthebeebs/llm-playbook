version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
      - ../models:/models  # Shared models directory
    environment:
      # Number of parallel requests (adjust for multi-user)
      - OLLAMA_NUM_PARALLEL=4

      # Max models to keep loaded (set to 1 for max performance)
      - OLLAMA_MAX_LOADED_MODELS=2

      # How long to keep models in memory (5m, 1h, -1 for always)
      - OLLAMA_KEEP_ALIVE=5m

      # Enable GPU
      - NVIDIA_VISIBLE_DEVICES=all

      # Debug mode (set to 1 for verbose logs)
      # - OLLAMA_DEBUG=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - ollama-network
    # Uncomment to run a model automatically on startup
    # command: serve

  # Optional: Stable Diffusion for image generation alongside Ollama
  # Uncomment the section below to enable
  # stable-diffusion:
  #   image: ghcr.io/AbdBarho/stable-diffusion-webui:latest-cuda
  #   container_name: ollama-stable-diffusion
  #   restart: unless-stopped
  #   ports:
  #     - "7860:7860"
  #   volumes:
  #     - ../models/stable-diffusion:/data/models/Stable-diffusion
  #     - ../models/stable-diffusion/lora:/data/models/Lora
  #     - ../models/stable-diffusion/vae:/data/models/VAE
  #     - ./sd-outputs:/output
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - CLI_ARGS=--listen --api --xformers --enable-insecure-extension-access --no-half-vae
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   networks:
  #     - ollama-network

  # Open WebUI - ChatGPT-like interface for Ollama
  # Recommended: Provides a great user experience!
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: ollama-webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - ./open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - ENABLE_OLLAMA_API=true
      # Uncomment if you enabled stable-diffusion above
      # - ENABLE_IMAGE_GENERATION=true
      # - IMAGE_GENERATION_ENGINE=automatic1111
      # - AUTOMATIC1111_BASE_URL=http://stable-diffusion:7860
    depends_on:
      - ollama
    networks:
      - ollama-network

networks:
  ollama-network:
    driver: bridge

# Ollama - Easiest Local LLM Platform
#
# Access API: http://localhost:11434
#
# Quick Start:
# 1. Start container: docker-compose up -d
# 2. Pull a model: docker exec -it ollama ollama pull llama3.1
# 3. Run interactively: docker exec -it ollama ollama run llama3.1
# 4. Or use API (see USAGE.md)
#
# Popular Models:
# - llama3.1 (8B, default)
# - llama3.1:70b (70B, best quality for RTX 4090)
# - mistral (7B, fast)
# - codellama (7B-70B, coding)
# - llama3.2-vision (11B, vision)
# - qwen2.5:72b (72B, multilingual)
#
# Model Management:
# - Pull: docker exec -it ollama ollama pull <model>
# - List: docker exec -it ollama ollama list
# - Remove: docker exec -it ollama ollama rm <model>
# - Info: docker exec -it ollama ollama show <model>
#
# Performance Tuning:
# - OLLAMA_NUM_PARALLEL: 1 (single user) to 8+ (multi-user)
# - OLLAMA_MAX_LOADED_MODELS: 1 (max speed) to 3+ (convenience)
# - OLLAMA_KEEP_ALIVE: -1 (always loaded) or 5m (auto-unload)
#
# For RTX 4090 (24GB):
# - Can run 70B Q4 models
# - Can run multiple 7-13B models simultaneously
# - Automatically offloads to RAM if needed (128GB available)
#
# Models stored in: ./models/
# Shared models with other engines: ../models/
#
# =====================================================================
# OPTIONAL SERVICES (Uncomment to enable)
# =====================================================================
#
# Stable Diffusion WebUI:
# - Adds AI image generation alongside Ollama
# - Access at http://localhost:7860
# - Shared GPU with Ollama (RTX 4090 can handle both)
#
# Open WebUI:
# - ChatGPT-like interface for Ollama
# - Access at http://localhost:3000
# - ENABLED BY DEFAULT (comment out to disable)
# - Supports text chat + image generation (if SD enabled)
#
# Stable Diffusion:
# - Uncomment the stable-diffusion service above to enable
# - Then uncomment the IMAGE_GENERATION env vars in open-webui
# - Access SD directly at http://localhost:7860
#
# Combined Setup (Ollama + Stable Diffusion + Web UI):
# - Text generation via Ollama
# - Image generation via Stable Diffusion
# - Unified interface via Open WebUI
# - All running on single RTX 4090
