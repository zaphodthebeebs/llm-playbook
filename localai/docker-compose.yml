version: '3.8'

services:
  localai:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    container_name: localai
    restart: unless-stopped
    ports:
      - "8082:8080"
    volumes:
      - ./models:/models
      - ./images:/tmp/generated/images
    environment:
      - DEBUG=true
      - MODELS_PATH=/models
      - THREADS=8
      - CONTEXT_SIZE=8192
      # GPU acceleration
      - NVIDIA_VISIBLE_DEVICES=all
      - BUILD_TYPE=cublas
    command: ["--models-path", "/models", "--context-size", "8192", "--threads", "8"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# LocalAI can download models automatically
# Or place GGUF models in ./models/
#
# Example model download (inside container):
# curl http://localhost:8080/models/apply \
#   -H "Content-Type: application/json" \
#   -d '{"id": "llama-3.1-8b-instruct"}'
#
# Or manually place GGUF files in ./models/
# Create a YAML config: models/model-name.yaml
