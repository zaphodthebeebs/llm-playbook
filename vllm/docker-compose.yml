version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ../models:/models
    environment:
      - HF_HOME=/root/.cache/huggingface
      # Set your HuggingFace token for gated models
      # - HF_TOKEN=hf_xxxxxxxxxxxxx
    command:
      - --model
      - meta-llama/Llama-3.1-70B-Instruct
      - --dtype
      - auto
      - --max-model-len
      - "8192"
      - --gpu-memory-utilization
      - "0.9"
      - --port
      - "8000"
      - --host
      - "0.0.0.0"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: '4gb'
    networks:
      - vllm-network

  # Open WebUI - ChatGPT-like interface for vLLM
  # Enabled by default - comment out to disable
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: vllm-webui
    restart: unless-stopped
    ports:
      - "3001:8080"
    volumes:
      - ./open-webui:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=dummy
      - ENABLE_OLLAMA_API=false
    depends_on:
      - vllm
    networks:
      - vllm-network

networks:
  vllm-network:
    driver: bridge

# vLLM - High-Performance LLM Inference Server
#
# API: http://localhost:8000/v1 (OpenAI-compatible)
# Web UI: http://localhost:3001 (Open WebUI - ChatGPT-like interface)
#
# Quick Start:
# 1. Edit model name in command section above
# 2. docker-compose up -d
# 3. Wait for model download (first start)
# 4. Access Web UI at http://localhost:3001
# 5. Or use API with base_url="http://localhost:8000/v1"
#
# To use a different model, change the --model parameter:
# Recommended models:
# - meta-llama/Llama-3.1-70B-Instruct (70B, needs AWQ/GPTQ)
# - meta-llama/Llama-3.1-13B-Instruct (13B, full FP16)
# - mistralai/Mistral-7B-Instruct-v0.3 (7B, fast)
# - Qwen/Qwen2.5-72B-Instruct (72B, excellent)
# - TheBloke/Llama-2-70B-Chat-AWQ (70B quantized, fits in 24GB)
#
# For AWQ/GPTQ quantized models (recommended for 70B):
# - Add: --quantization awq
# - Or: --quantization gptq
#
# Performance tuning:
# - --gpu-memory-utilization: 0.8-0.95 (default 0.9)
# - --max-model-len: Reduce if OOM (e.g. 4096 instead of 8192)
# - --tensor-parallel-size: For multi-GPU (e.g. 2 for 2xGPUs)
#
# HuggingFace token (for gated models like Llama):
# - Uncomment HF_TOKEN in environment
# - Get token from https://huggingface.co/settings/tokens
