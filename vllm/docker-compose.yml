services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    ports:
      - "8090:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ../models:/models
    env_file:
      - .env
    environment:
      - HF_HOME=/root/.cache/huggingface
      - HF_TOKEN=${HF_TOKEN:-}
    command:
      - --model
      - Qwen/Qwen2.5-7B-Instruct
      - --dtype
      - auto
      - --max-model-len
      - "8192"
      - --gpu-memory-utilization
      - "0.9"
      - --port
      - "8000"
      - --host
      - "0.0.0.0"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: '4gb'
    networks:
      - vllm-network

  # Open WebUI - ChatGPT-like interface for vLLM
  # Enabled by default - comment out to disable
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: vllm-webui
    restart: unless-stopped
    ports:
      - "3001:8080"
    volumes:
      - ./open-webui:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=dummy
      - ENABLE_OLLAMA_API=false
    depends_on:
      - vllm
    networks:
      - vllm-network

networks:
  vllm-network:
    driver: bridge

# vLLM - High-Performance LLM Inference Server
#
# API: http://localhost:8090/v1 (OpenAI-compatible)
# Web UI: http://localhost:3001 (Open WebUI - ChatGPT-like interface)
#
# REQUIREMENTS:
# - NVIDIA GPU with CUDA 12.9+ support (driver 575+)
# - Check with: nvidia-smi (CUDA Version shown top right)
#
# IMPORTANT: Unlike Ollama, vLLM does NOT support downloading models from the UI.
# Models must be specified in the --model command above and are downloaded
# automatically when the container starts. To change models, edit docker-compose.yml
# and restart the container.
#
# Quick Start:
# 1. Create .env file (optional, for gated models like Llama)
# 2. Edit model name in command section above if desired
# 3. docker-compose up -d
# 4. Wait for model download (first start can take several minutes)
# 5. Access Web UI at http://localhost:3001
# 6. Select the model from the dropdown (ignore "Manage Models" - that's for Ollama)
# 7. Or use API with base_url="http://localhost:8090/v1"
#
# Example .env file (for gated models like Llama):
# HF_TOKEN=hf_your_huggingface_token_here
#
# Get your token from: https://huggingface.co/settings/tokens
# Then accept model license at the model's HuggingFace page
#
# Recommended models (ungated - no HF token needed):
# - Qwen/Qwen2.5-7B-Instruct (7B, fast, good quality)
# - Qwen/Qwen2.5-14B-Instruct (14B, better quality)
# - mistralai/Mistral-7B-Instruct-v0.3 (7B, fast)
#
# Gated models (require HF token + license acceptance):
# - meta-llama/Llama-3.1-8B-Instruct
# - meta-llama/Llama-3.1-70B-Instruct (needs quantization for 24GB)
#
# For AWQ/GPTQ quantized models (for large models on limited VRAM):
# - Add: --quantization awq
# - Or: --quantization gptq
#
# Performance tuning:
# - --gpu-memory-utilization: 0.8-0.95 (default 0.9)
# - --max-model-len: Reduce if OOM (e.g. 4096 instead of 8192)
# - --tensor-parallel-size: For multi-GPU (e.g. 2 for 2xGPUs)
